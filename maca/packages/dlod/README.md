# Caffe-based Cell Detection Experiment

This module includes code and models which together provide a procedure for identifying structures in neural image data.  In particular, we focus on dense (i.e. per-pixel) classification problems where the goal is to discriminate between cell body pixels and non-cell body pixels.  Our approach is based on [this paper](http://papers.nips.cc/paper/4741-deep-neural-networks-segment-neuronal-membranes-in-electron-microscopy-images) where convolutional neural networks (CNNs) are applied in a sliding window manner to segment electron microscopy (EM) images.


## Prerequisites

In order to run this code you will need the following:

- The Caffe deep learning framework, compiled to create both the command line executable and the python interface.  We tested using Caffe [version 1.0, release candidate 2](https://github.com/BVLC/caffe/releases) (dated 15 Feb 2015).   Note that there is an error in the memory data layer in this version of Caffe, and this must be patched.  See [here](https://github.com/BVLC/caffe/issues/2334) for a general description of the problem and [here](https://github.com/TJKlein/caffe/commit/5f1bb97a587043dbe0892466b866abfe4c76804c) for detailed fix instructions.

- Python version 2.7 or better with all the standard scientific computing packages (e.g. numpy, pylab, skimage).  We used [anaconda](https://www.continuum.io/downloads) 1.7.0 which provides Python 2.7.5.  You will also need to install the LMDB python interface (easy to do with pip - instructions are [here](https://lmdb.readthedocs.org/en/release/)).

We recommend using a GPU-enabled system running a linux-based OS.  This code was tested on a Redhat (CentOS) system equipped with GeForce GTX TITAN cards.

## Quick Start

There are a few basic steps:

1. Configure the provided Makefile (assuming you want to run the examples included with this package).  This involves changing the paths at the top of the Makefile; also, if you want to run with a CPU instead of a GPU, you'll need to remove lines of the form "--gpu X".

2. Setup a binary classification problem.  This involves extracting labeled examples from annotated neuro data images and saving these examples in a Caffe-compatible LMDB database.  For the provided example, this can be accomplished via
    ```make lmdb```
which will generate some status to stdout and ultimately report some statistics for the training and test datasets.  To create LMDB databases for your problem of interest, call the [make_binary_dataset.py](./src/make_binary_dataset.py) script with appropriate *--train-dir* and *--test-dir* arugments (see the make target for an example of the arguments this script expects).

3. Train a Caffe classifier.  For best results, you will need to design the neural network architecture and select reasonable hyperparameters.  This is a non-trivial exercise and is typically problem dependent.  As part of this package we provide a simple example derived from one of the models included in Caffe (originally for CIFAR-10) that uses the LMDB dataset created in the previous step.  To train this model, do
    ```make train-gpu```.
By default this runs Caffe (from the command line) for 60000 training iterations.  The model structure and hyperparameters can be customized by modifying the prototxt files in [this](./models/cifar10) directory.  Note that it will take some time to train the model (order tens of minutes, depending on your system/GPU).

4.  Apply the classifier to new data.  Since it is not terribly convenient to extract estimates via the Caffe command-line tool, we provide a Python script that uses the Python Caffe interface to process the image and extract per-pixel estimates.  For an example of how to run this script using the classifier developed in the previous step do 
```make deploy-gpu```.  This target can be modified in a straightforward way to process a different image, use a different Caffe model, etc.


## Additional Details

### Ground Truth
The performance of the CNN classifier will be a function of the classifier design and also the data used to train the classifier. The [existing](./data/orig) cell body ground truth (generated by external domain experts) is somewhat "sparse" in that each cell body has only a single annotated pixel and no non-cell pixels are annotated.  To construct a binary classification problem we require examples from both classes; furthermore, most deep learning-based approachs require a fairly large number of examples.  However, not all examples are equally useful, and care should be taken to construct truth that is most representative of the problem of interest.

For example, consider the "sparsely" annotated image below, where the individual bright pixels (those with value 255) denote a cell body.

![alt text](./data/ForDocs/Y_sparse.png?raw=true, "Original Annotations")

One approach to generate additional examples is to expand the sparse class labels by labeling nearby pixels (where "nearby" could mean in Euclidean distance, pixel intensity or some other metric) as cell bodies and labeling those pixels sufficiently far away as non-cell.  For example, [this](./src/truth_image.m) script generates a more extensive set of putative ground truth labels by automatically growing the sparse class labels to a larger set of pixels.  An example output from this script is shown below:

![alt text](./data/ForDocs/Y_inferred.png?raw=true, "Inferred Annotations")

where cell body pixels are labeled +1, non-cell body pixels have label -1 and pixels without a truth label have the value 0 (there are only three values in this image; the colorbar is only provided to make clear which pixels take which values).  Note that, in order to reduce label noise, the script above does not attempt to extend the initial labeling to every pixel; using different parameters when running the script will lead to more or less conservative labelings.  This procedure leads to abundant (albeit  less accurate) labels.  Training a classifier using labels generated in this manner (across all images in the training data set) and then applies the resulting classifier to the following test image:

![alt text](./data/interpolated/img.00012.interp.png?raw=true, "Test Image")

one obtains class estimates of the form (colors indicate probability of cell body at the given pixel):

![alt text](./data/ForDocs/Yhat_gross.png?raw=true, "Inferred Annotations")

These estimates*, while consistent with the test image, are clearly not well-suited for fine-grained discrimination of cell boundaries (appears to substantially overestimate the cell body extent).  This is likely an artifact of:

- uniformly sampling from the entire space of examples, which tends to draw pixels that are trivial to classify and
- the automated pixel labeling procedure too aggressively labeling pixels near the true annotations as cell bodies.

For more useful estimates, one can explicitly design the ground truth to place greater emphasis on structures of interest.  For example, if one trains a classifier using the manual annotations below (which have a higher proportion of more accurate labels near cell boundaries; the cell and non-cell truth labels are shown in magenta and cyan):

![alt text](./data/ForDocs/Y_manual1.png?raw=true) 

the resulting estimates produce a qualitatively more desirable result with respect to cell boundaries (for the same test image; compare to the first set of estimates above):

![alt text](./data/ForDocs/Yhat_manual1.png?raw=true)

Of course, these estimates are still likely suboptimal; providing ground truth that focuses upon cell boundaries will likely result in estimates that represent these boundaries more faithfully.  Ultimately, it is best if a domain expert generates these annotations with an eye towards providing the most interesting/difficult examples for the classifier (less interesting cases of extremely bright or extremely dark pixels can be labeled using some simpler procedure, such as applying a threshold).

*Note: Class estimates are only generated for pixels sufficiently far in the interior of the image; pixels near the border do not have enough context and are not classified at this time (although you could mirror nearby pixels to generate additional context if desired).
