# Caffe-based Cell Detection Experiment

This module provides code and models demonstrating a procedure for identifying structures in neural image data.  In particular, we focus on dense (i.e. per-pixel) classification problems where the goal is to discriminate between cell body pixels and non-cell body pixels.  The general approach is to apply a sliding window detector based on convolutional neural networks (CNNs); this technique has been used to solve similar problems in the past, e.g. [membrane detection](http://papers.nips.cc/paper/4741-deep-neural-networks-segment-neuronal-membranes-in-electron-microscopy-images).
  

## Prerequisites

In order to run this code you will need the following:

- The Caffe deep learning framework, compiled to create both the command line executable and the python interface.  We tested using Caffe [version 1.0, release candidate 2](https://github.com/BVLC/caffe/releases) (dated 15 Feb 2015).   Note that there is an error in the memory data layer in this version of Caffe, and this must be patched.  See [here](https://github.com/BVLC/caffe/issues/2334) for a general description of the problem and [here](https://github.com/TJKlein/caffe/commit/5f1bb97a587043dbe0892466b866abfe4c76804c) for detailed fix instructions.

- Python version 2.7 or better with all the standard scientific computing packages (e.g. numpy, pylab, skimage).  We used [anaconda](https://www.continuum.io/downloads) 1.7.0 which provides Python 2.7.5.  You will also need to install the LMDB python interface (easy to do with pip - instructions are [here](https://lmdb.readthedocs.org/en/release/)).

We recommend using a GPU-enabled system running a linux-based OS.  This code was tested on a Redhat (CentOS) system equipped with GeForce GTX TITAN cards.

## Quick Start

There are a few basic steps:

1. Configure the provided Makefile (assuming you want to run the examples included with this package).  This involves changing the paths at the top of the Makefile; also, if you want to run with a CPU instead of a GPU, you'll need to remove lines of the form "--gpu X".

2. Setup a binary classification problem.  This involves extracting labeled examples from annotated neuro data images and saving these examples in a Caffe-compatible LMDB database.  For the provided example, this can be accomplished via
    ```make lmdb```
which will generate some status to stdout and ultimately report some statistics for the training and test datasets.

3. Train a Caffe classifier.  For best results, you will need to design the neural network architecture and select reasonable hyperparameters.  This is a non-trivial exercise and is typically problem dependent.  As part of this package we provide a simple example derived from one of the models included in Caffe (originally for CIFAR-10) that uses the LMDB dataset created in the previous step.  To train this model, do
    ```make train-gpu```.
By default this runs Caffe (from the command line) for 60000 training iterations.  The model structure and hyperparameters can be customized by modifying the prototxt files in [this](./models/cifar10) directory.  Note that it will take some time to train the model (order tens of minutes, depending on your system/GPU).

4.  Apply the classifier to new data.  Since it is not terribly convenient to extract estimates via the Caffe command-line tool, we provide a Python script that uses the Python Caffe interface to process the image and extract per-pixel estimates.  For an example of how to run this script using the classifier developed in the previous step do 
```make deploy-gpu```.  This target can be modified in a straightforward way to process a different image, use a different Caffe model, etc.


## Additional Details

### The Importance of Ground Truth
The performance of the CNN classifier will be a function of the classifier design and also the data used to train the classifier. The [existing](./data/orig) cell body ground truth (generated by external domain experts) is somewhat "sparse" in that each cell body has only a single annotated pixel and no non-cell pixels are annotated.  To construct a binary classification problem we require examples from both classes.  For a deep learning-based approach, we also require a fairly large number of examples.  However, not all examples are equal, and care should be taken to construct truth that is most representative of the problem of interest.

For example, consider the "sparsely" annotated image below, where the individual bright pixels (those with value 255) denote a cell body.

![alt text](./data/ForDocs/Y_sparse.png?raw=true, "Original Annotations")

One approach to generate additional examples is to expand the sparse class labels by labeling nearby pixels (where "nearby" could mean in Euclidean distance, pixel intensity or some other metric) as cell bodies and labeling those pixels sufficiently far away as non-cell.  For example, using [this](./src/truth_image.m) script one obtains a more extensive set of putative ground truth labels:

![alt text](./data/ForDocs/Y_inferred.png?raw=true, "Inferred Annotations")

where cell body pixels are labeled +1, non-cell body pixels have label -1 and a set of "keep out" pixels are labeled 0 (the goal of the "keep out" region is to reduce label noise).  This procedure leads to abundant (albeit possibly less accurate) labels - too many in fact to use them all.  If we then uniformly subsample from this set of artificially generated labels (across all images in the training data set) and then applies the resulting classifier to the following test image:

![alt text](./data/interpolated/img.00012.interp.png?raw=true, "Test Image")

one obtains class estimates of the form:

![alt text](./data/ForDocs/Yhat_gross.png?raw=true, "Inferred Annotations")

These estimates*, while reasonably well-aligned with the test image, are clearly not well-suited for fine-grained discrimination of cell boundaries (appears to substantially overestimate the cell body extent).  This is likely an artifact of:

- uniformly sampling from the entire space of examples, which tends to draw pixels that are trivial to classify and
- the automated pixel labeling procedure too aggressively labeling pixels near the true annotations as cell bodies.

For more useful estimates, one can explicitly design the ground truth to place greater emphasis on the structure of interest.  For example, if one trains a classifier using the manual annotations below (which have a higher proportion of more accurate labels near cell boundaries; the cell and non-cell truth labels are shown in magenta and cyan):

![alt text](./data/ForDocs/Y_manual1.png?raw=true) 

the resulting estimates produce a qualitatively more desirable result with respect to cell boundaries (for the same test image; compare to the first set of estimates above):

![alt text](./data/ForDocs/Yhat_manual1.png?raw=true)

Of course, these estimates are still likely suboptimal; a more "aggressive" ground truth data set (e.g. that has smaller "keep out" regions between cell and non-cell pixels) will likely result in estimates that represent cell boundaries more faithfully.  Ultimately, it is best if a domain expert generates these annotations with an eye towards providing the most interesting/difficult examples for the classifier (less interesting cases of extremely bright or extremely dark pixels can be labeled using some simpler procedure, such as applying a threshold).

*Note: Class estimates are only generated for pixels sufficiently far in the interior of the image; pixels near the border do not have enough context and are not classified at this time (although you could mirror nearby pixels to generate additional context if desired).